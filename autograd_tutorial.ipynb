{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "        # PyTorch Autograd: Hands-on Guide\n",
        "        Short, runnable tour of how gradients flow in PyTorch. Run each cell and watch grads populate.\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29ca9ea9",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "        ## Core ideas to repeat aloud\n",
        "        - Set `requires_grad=True` on tensors you want gradients for; PyTorch records ops into a graph.\n",
        "        - Call `.backward()` on a scalar loss to traverse the graph and populate `.grad` on leaves.\n",
        "        - Gradients accumulate by default; clear them with `.zero_grad()`.\n",
        "        - Skip tracking during evaluation or logging with `torch.no_grad()` or by `.detach()`-ing a tensor.\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch version: 2.9.1\n",
            "device: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "torch.manual_seed(0)\n",
        "print('torch version:', torch.__version__)\n",
        "print('device:', 'cuda' if torch.cuda.is_available() else 'cpu')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "        ## Scalar example: verify the derivative by hand\n",
        "        Forward: `y = x**2 + 2x + 1`. At `x = 3`, dy/dx = `2x + 2 = 8`.\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "y value: 16.0\n",
            "dy/dx at x=3: 8.0\n"
          ]
        }
      ],
      "source": [
        "x = torch.tensor(3.0, requires_grad=True)\n",
        "y = x**2 + 2 * x + 1\n",
        "print('y value:', y.item())\n",
        "\n",
        "y.backward()  # populates x.grad\n",
        "print('dy/dx at x=3:', x.grad.item())\n",
        "\n",
        "# 然后可以再给同学们看一看Linear modal里面的parameters，在一个step之后，梯度的变化。这样很直观\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "        ## Vector example: linear layer + MSE loss\n",
        "        Build a tiny linear model `y = Xw + b`, compute mean squared error, and inspect gradient shapes.\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "        w = torch.randn(2, 1, requires_grad=True)\n",
        "        b = torch.zeros(1, requires_grad=True)\n",
        "\n",
        "        X = torch.tensor([[1.0, 2.0],\n",
        "                          [3.0, 4.0]])\n",
        "        y_true = torch.tensor([[1.0],\n",
        "                               [2.0]])\n",
        "\n",
        "        y_pred = X @ w + b  # matrix multiply + bias broadcast\n",
        "        loss = ((y_pred - y_true) ** 2).mean()\n",
        "        print('loss:', loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        print('w.grad shape:', w.grad.shape)\n",
        "        print('b.grad shape:', b.grad.shape)\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "        ## Gradient accumulation vs. zeroing\n",
        "        Calling `.backward()` again **adds** to existing grads. Clear them with `.zero_grad()` (common before each optimizer step).\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "        loss2 = ((X @ w + b - y_true) ** 2).mean()\n",
        "        loss2.backward()\n",
        "        print('grad after second backward (accumulated):', w.grad.flatten())\n",
        "\n",
        "        # Reset\n",
        "        w.grad.zero_()\n",
        "        b.grad.zero_()\n",
        "        print('grads after zeroing:', w.grad.flatten(), b.grad)\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "        ## Turning off tracking: `torch.no_grad()` and `detach()`\n",
        "        Useful for evaluation, logging, or breaking the graph when you do custom manipulations.\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "        with torch.no_grad():\n",
        "            preds = X @ w + b\n",
        "        print('preds without tracking require_grad?', preds.requires_grad)\n",
        "\n",
        "        # Detach produces a tensor that shares storage but has no grad history\n",
        "        detached = (X @ w + b).detach()\n",
        "        print('detached requires_grad?', detached.requires_grad)\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "        ## Mini training loop demo\n",
        "        Fit a 1D linear regression `y = 3x + 2` with noise to show loss decreasing and gradients flowing.\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "        torch.manual_seed(0)\n",
        "        x_train = torch.linspace(-2, 2, steps=50).unsqueeze(1)\n",
        "        y_train = 3 * x_train + 2 + 0.3 * torch.randn_like(x_train)\n",
        "\n",
        "        model = torch.nn.Linear(1, 1)\n",
        "        optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "        loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "        for step in range(1, 41):\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(x_train)\n",
        "            loss = loss_fn(preds, y_train)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if step % 10 == 0:\n",
        "                print(f'step {step:02d} loss {loss.item():.4f} | weight {model.weight.item():.3f} bias {model.bias.item():.3f}')\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "        ## Custom autograd Function (lightweight intro)\n",
        "        Most layers are built-in, but you can define both forward and backward for custom ops.\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "        class SquarePlusOne(torch.autograd.Function):\n",
        "            @staticmethod\n",
        "            def forward(ctx, input):\n",
        "                ctx.save_for_backward(input)\n",
        "                return input * input + 1\n",
        "\n",
        "            @staticmethod\n",
        "            def backward(ctx, grad_output):\n",
        "                (input,) = ctx.saved_tensors\n",
        "                grad_input = grad_output * 2 * input\n",
        "                return grad_input\n",
        "\n",
        "        x_demo = torch.tensor([2.0, -3.0], requires_grad=True)\n",
        "        y_demo = SquarePlusOne.apply(x_demo).sum()\n",
        "        y_demo.backward()\n",
        "        print('x_demo grad (expected 2*x):', x_demo.grad)\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "        ## Takeaways to emphasize\n",
        "        - Always start from a scalar loss when calling `.backward()`.\n",
        "        - Clear grads each step; accumulating is rarely what you want.\n",
        "        - Use `torch.no_grad()` for eval/logging to save memory and avoid accidental graph building.\n",
        "        - Inspect `.grad` shapes to debug mismatch issues early.\n",
        "        "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "machinelearningysu2025 (3.13.9)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
